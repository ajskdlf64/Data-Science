{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 기반 기계번역\n",
    "\n",
    " - 기계 번역은 인간이 사용하는 자연 언어를 컴퓨터를 사용하여 다른 언어로 번역하는 일을 말한다.\n",
    "\n",
    "\n",
    " - 일관성과 통일성이 있게 번역되는 장점이 있으나 기계 번역은 현재로서는 자연스러운 번역을 보장할 수 없으며, 문장이 부자연스러운 어투로 나오는 경우가 잦다. 하지만 기술적인 진보가 계속해서 이루어지고 있으며 통계 및 인공 지능과 컴퓨터 처리 능력의 발전으로 점점 더 빠르게 성장하고 있다.\n",
    "\n",
    "\n",
    " - 기계 번역의 개념 자체는 컴퓨터의 존재 이전부터 존재하기 때문에 기계 번역은 번역 소프트웨어와 동의어는 아니지만 현재 기계 번역은 대부분 번역 소프트웨어로 구현된다. 예를 들어, 영어 문장을 입력하면 그것을 번역하는 한국어 문장을 출력하는 영한 번역 소프트웨어 등이 있다.\n",
    "\n",
    "#### Neural Machine Translation\n",
    " - Input : 소스 언어(Source Language), 번역의 원천이 되는 언어\n",
    " - Output : 타겟 언어(Target Language), 번역을 하고자 하는 언어\n",
    " \n",
    "\n",
    "#### 문제정의\n",
    " - Input 문장이 주어졌을 때, 번역된 문장을 출력하기 때문에 지도 학습이다.\n",
    " - Input 문장이 주어졌을 때, 번역될 문장의 조건부 확률 분포를 계산하게 된다.\n",
    " - $P(Y=(y_1,...,y_T) | X=(x_1,...,x_{T'}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder & Decoder\n",
    "- 소스 언어와 타겟 언어의 유니크한 단어장을 구축한다.\n",
    "- 토크나이즈(tokenize) : 단어 및 쉼표, 마침표 등을 분리하고, 부호를 표준화(통일) 시킨다.\n",
    "- 하위 단어의 세분화(subword segmentation) : BPE 인코딩을 통해 하위 단어의 세분화를 진행한다.\n",
    "- 모든 하위 단어를 통합하여 빈도수 내림차순으로 정렬한 뒤 인덱스를 부여한다.\n",
    "\n",
    "\n",
    "#### 인코더(Encoder)\n",
    "- 소스 문장의 토큰들을 문장을 표현하는 벡터의 집합으로 인코딩한다.\n",
    "- 벡터의 집합을 크기가 고정된 하나의 벡터로 만들 필요는 없다.\n",
    "   - 이유 1 : 하나의 벡터로 합치게 되면 정보 손실이 발생한다.\n",
    "   - 이유 2 : 문장의 길이가 길어질수록 훈련하기도 힘들고 모델의 크기를 키워야하기 때문이다.\n",
    "   \n",
    "   \n",
    "#### 디코더(Decoder)\n",
    "- 자기회귀 언어 모델링(Autoregressive Language Modeling)을 활용하고 문맥(Context)을 무한대로 보게 된다.\n",
    "   - Recurrent Networks, Self-Attention, (diated) Convolutional Network 등을 사용한다.\n",
    "- 문장의 의미 뿐만 아니라 문법적인 구조도 함께 봐야하기 때문에 인과관계 구조 (Casual Structure)를 따라야 한다.\n",
    "- 조건부 언어 모델링(Conditional Language Modeling)\n",
    "   - 이전에 출력된 단어와 인코딩된 소스 문장, 두 가지를 동시에 보고 다음에 어떤 단어가 나올지 결정하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Neural Machine Translation\n",
    " - 소스 문장의 표현(Source sentence representation)은 양방향 RNN(bidirectional RNN)을 활용한다.\n",
    " - 타겟 토큰의 표현은 단일 방향 RNN(undirectional RNN)을 활용한다.\n",
    " - Attention mechanism : 다음 토큰을 예측하기 전에 소스 문장에서 어느 토큰이 제일 연관성이 높은가를 살펴보게 된다.\n",
    " - 연관성을 나타내는 벡터와 인코딩된 소스 문장 벡터와 결합하여 소스 문맥 벡터(Context vector)를 도출한다.\n",
    " - 다음 타겟 토큰의 예측을 위해 소스 문맥 벡터와 이전 타겟 토큰 벡터를 함께 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
