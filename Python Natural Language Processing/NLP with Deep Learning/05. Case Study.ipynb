{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Describe Multimedia\n",
    "- Input 굳이 문장, 텍스트여야만 하는가?\n",
    "  - Input 데이터를 연속 벡터 공간(Continious vector space)에 인코딩 하기만 하면, 어떤 Input 데이터든 상관이 없다.\n",
    "  - 인코딩 된 벡터는 사람의 눈으로 볼 수 없는 방식으로 중요 요소만 남아있기 때문에 여러 멀티미디어로 확장할 수 있다.\n",
    "  \n",
    "\n",
    "- Image Caption Generation\n",
    " - Input : 이미지\n",
    " - Output : 이미지 캡션\n",
    " - 아키텍처\n",
    "    - 인코더(Encoder) : Deep convolution network\n",
    "    - 디코더(Decoder) : Recurrent language model + attention machanism\n",
    "    \n",
    "    \n",
    "- Video Description Generation\n",
    " - Input : 동영상 클립(일련의 비디오 프레임)\n",
    " - Output : 동영상에 대한 설명 문장\n",
    " - 아키텍처\n",
    "    - 인코더(Encoder) : Deep2 + 3D convolution network\n",
    "    - 디코더(Decoder) : Recurrent language model + attention machanism\n",
    "        \n",
    "    \n",
    "- Speech recognition\n",
    " - Input : 스피치(음성 데이터)\n",
    " - Output : 음성에 대한 자막\n",
    " - 아키텍처\n",
    "    - 인코더(Encoder) : Convolution + recurrent acoustic network\n",
    "    - 디코더(Decoder) : Conditional recurrent language model + attention machanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Character-Level Machine Translation\n",
    "- 글자 단위의 기계번역을 학습한다. 또한 실험결과를 통해 다중언어 기계번역으로 확장시킨다.\n",
    "\n",
    "\n",
    "\n",
    "- 왜 하위 단어 단위(Sub word-level)의 모델링은 힘들까?\n",
    " - 형태(morphology)가 풍부한 언어는 단어 수가 많아지기 때문에 다루기 어렵다. 특히 합성어가 많을 경우 더 어렵다.\n",
    " - 줄임말, 오타를 다루기 어렵다.\n",
    " - 정보량이 다른 토큰에 같은 파라미터를 부여하기 때문에, 모델링이 효율적이지 못한다.\n",
    " \n",
    "\n",
    "\n",
    "- 글자 단위 모델링(character-level modeling)에 대한 의구심\n",
    " - 과연 길고 말이 되는 문장을 생성할 수 있는가?\n",
    " - 단어들은 높은 수준의 요약이 되어 있는데, 글자에서 단어로 만들어 지는 과정은 더 복잡한 비선형 프로세스다. 이것을 기계학습이 가능할까?\n",
    " - 효율적으로 학습이 가능할까?\n",
    " \n",
    " \n",
    " \n",
    "#### Fully Character Machine Translation\n",
    "- 문제\n",
    " - 맞춤법은 특정한 패턴이 없다. 글자를 의미가 있는 단어로 만들기 위해 패턴을 잡아내는 고민을 해야 한다.\n",
    " - Self-Attention을 사용하기 때문에, 복잡도가 크게 올라간다.\n",
    "- 문제를 해결하기 위한 모델구조\n",
    " - Multi-Width Convolution for a character sequence > max polling > \n",
    "   high-way network for nonlinear map ping > attention over a convolution feature map\n",
    "\n",
    "\n",
    "\n",
    "- Character-Level Multilingual Translation\n",
    " - 여러 언어를 글자 단위로 Input으로 넣어서 학습시킨다.\n",
    " - 서로 다른 언어의 공통적인 구조를 연속 벡터 공간(Contunuous vectro space)에 투영되었다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning of Low-Resource Neural Machine Translation\n",
    "\n",
    "\n",
    "#### MultiTask learning\n",
    " - 문제점\n",
    "  - 적은 데이터에 대해서는 과적합이 되는데, 풍부한 데이터에 대해서는 과소적합이 되는 현상이 있다.\n",
    "  - 극히 전은 데이터는 무시하는 경향이 있다.\n",
    " - 다중 언어 기계번역을 굉장히 적은 데이터로 학습시키려고 했지만, 전이학습으로는 잘 안됐다.\n",
    " \n",
    "\n",
    "#### Meta learning\n",
    " - 목표 : 이전과 달리 새로운 문제가 생겼을 때 더 잘 풀 수 있게 하는 것을 목표로 한다.\n",
    "\n",
    "- 과정\n",
    "  - 1. Simulated Learning\n",
    "      훈련 데이터 세트 Task 묶음에서 미니배치 데이터를 선택한 후, 현재 매개변수에서 확률적 경사 하강법(SGD)을 시뮬레이션 하고 어떻게 되는지 살펴본다.\n",
    "  - 2. Meta Learning\n",
    "      검증 세트의 loss 값이 가장 낮게 만드는 목적으로 파라미터를 업데이트 한다. 즉 gradient의 gradient(=hessian)을 구해야 한다.\n",
    "  - 3. Fast adaptation to a new task\n",
    "\n",
    "\n",
    "- 훈련 완료 후 앞으로 새로운 task를 훈련시킬 때, 랜덤하게 매개변수를 시작하는 것 보다 훨씬 더 잘된다.\n",
    "\n",
    "\n",
    "- 장점\n",
    "  - 새로운 문제가 생겼을 때 빠르게 적용할 수 있는 매개변수를 찾는다.\n",
    "  - 기존 task에 과적합이 덜 되게 할 수 있다.\n",
    " \n",
    " \n",
    "- 주의\n",
    "  - Input / Output miss match : 각각의 언어의 Vocabulary가 다르다.\n",
    "  - Universal lexical representation 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Translation Learning to Decode\n",
    "\n",
    "\n",
    "#### Decoding\n",
    " - 완전탐색(Exhaustive Search, Brute-force search)\n",
    "   - 가능한 경우를 모두 구해서 문제의 해결 방법을 찾는 것이다.\n",
    "   - 그러나, 물리적으로 불가능하다.\n",
    " - Ancestral Sampling, Forward Sampling\n",
    "   - 샘플을 많이 뽑아야하고, Variancerk shvek.\n",
    " - 그리디 탐색(Greedy Search)\n",
    "   - 가능성이 제일 높은 토큰 하나를 선택한다.\n",
    "   - 효율적이지만 차선책이다. 한번한 선택을 되돌리 수 없다는 단점이 있다.\n",
    " - 빔 탐색(Beam Search)\n",
    "   - 가능한 여러개의 후보자를 가지고 그들 중에서 최선의 선택을 한다.\n",
    "   - 후보자 개수가 올라간다고 해서 탐색 결과가 좋아지는 것은 아니다.\n",
    "   \n",
    " \n",
    "#### Decoding 목적이 불분명하다.\n",
    " - 실시간 번역 시스템이 목적이라면, 품질을 높여야 하지만 딜레이는 낮춰야 한다.\n",
    " - 어린이들을 위한 번역 시스템이 목적이라면, 품질은 높여야 하지만, 텍스트 난이도는 낮춰야 한다.\n",
    " - 온디바이스에서의 번역 시스템 구축이 목적이라면, 품질은 높아야 하지만 게산복잡도는 낮아야 한다.\n",
    " \n",
    " \n",
    "#### Learning to decode\n",
    " - 신경망이 무엇인가 생각해보면 forgeting machine 이라고 할 수 있다.\n",
    "   - Forward 과정에서 최대한 input에 대한 정보를 버리는 과정이다.\n",
    "   - 즉, 은닉 활성화 값(hidden activation)에는 판단을 하기 위해 이와 관련있거나 없는 특징을 구별할 수 있는 정보를 포함하고 있다.\n",
    " - RNN에서 은닉층 탐색을 해본 결과\n",
    "   - CNN과 달리 봐도 시각적으로 어떤 특정 패턴이 있는지 구별할 수 없다.\n",
    "   - 하지만 신경망은 이를 잘 활용할 수 있다는 점은 분명하다.\n",
    "   \n",
    "\n",
    "#### Trainable Decoding\n",
    " - 기존 conditional RNN을 환경으로 정의하게 되면, 강화학습의 알고리즘을 사용할 수 있다.\n",
    " - Decoder를 에이전트(agent)로 보고 훈련 가능한 decoding agent를 쌓는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
