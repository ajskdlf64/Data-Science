{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    " - Tensorflow는 구글에서 2015년에 오픈소스로 발표한 머신러닝 라이브러리다.\n",
    "\n",
    "\n",
    " - 일반인이 사용하기 쉽게 구성되어 있고, 파이썬을 주로 활용해 손쉽게 모델링 및 테스트 해 볼 수 있는 구조이다.\n",
    "\n",
    "\n",
    " - 또한 머신러닝 생태계를 중요시해서 적극적인 이슈 대응 및 버전 업그레이드를 통해 인지도를 높이고 있다.\n",
    " \n",
    " \n",
    " - Tensor는 N차원의 매트릭스를 의미하며, 말 그대로 Tensor(텐서)를 Flow(플로)한다는 것은 데이터 흐름 그래프(Data Flow Graph)를 사용해 수치 연산을 하는 과정을 의미한다.\n",
    " \n",
    " \n",
    " - 그래프의 Node는 수치 연산(operator), 변수(variable), 상수(constant)를 나타내고, edge는 노드 사이를 이동하는 다차원 데이터 배열(tensor)을 나타낸다.\n",
    "\n",
    "\n",
    " - 텐서플로는 유연한 아키텍처로 구성되어 있어 코드를 수정하지 않고도 데스크톱, 서버 혹은 모바일 디바이스에서 CPU나 GPU를 사용해 연산을 구동시킬 수 있다.\n",
    " \n",
    " \n",
    "#### Tensorflow의 특징\n",
    " - 데이터 플로우 그래프를 통한 풍부한 표현력\n",
    " - 아이디어 테스트에서 서비스 단계까지 이용 가능\n",
    " - 계산 구조와 목표 함수만 정의하면 자동으로 미분 계산을 처리\n",
    " - 파이썬/C++를 지원하며, SWIG를 통해 다양한 언어를 지원 가능\n",
    " - 유연성과 확장성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers\n",
    " - 텐서플로를 이용해 하나의 딥러닝 모델을 만드는 것은 마치 블록을 하나씩 쌓아서 전체 구조를 만들어 가는 과정이다.\n",
    " - 따라서, 쉽게 블록을 바꾸고, 여러 블록의 조합을 쉽게 만들 수 있다는 것이 장점이다.\n",
    " - tf.keras.layer는 이런 블록 역할을 하는 모듈이다.\n",
    " - keras는 텐서플로와 같은 딥러닝 오픈소스인데, 텐서플로에서도 케라스를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Dense\n",
    " - 첫번째 모듈이다.\n",
    " - Dense란 신경망 구조의 가장 기본적인 형태를 의미한다.\n",
    " - 즉 $y = f(Wx+b)$ 의 수식을 만족하는 기본적인 신경망 형태의 층을 만드는 함수이다.\n",
    " - 수식에서, x는 입력벡터, b는 편향벡터, W는 가중치 행렬이다. 즉 가중치와 입력벡터를 곱한 후 편향을 더해준다.\n",
    " - 그리고 그 값에 f 라는 활성화 함수를 적용하는 구조다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이러한 Dense 층을 구성하는 가장 기본적인 방법은 가중치 W와 b를 각각 변수로 선언한 후 행렬 곱을 통해 구하는 방법이다.\n",
    "\n",
    "import tensorflow as tf                                            # Tensorflow 라이브러리 호출\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([5,10], -1.0, 1.0))              # 5X10 행렬의 -1~1 사이의 균등분포 난수를 입력한 Tensor를 생성한다.\n",
    "b = tf.Variable(tf.zeros([10]))                                    # 10X1 행렬의 원소가 모두 0인 Tensor를 생성한다.\n",
    "\n",
    "# y = tf.matmul(W, x) + b                                          # 활성화 함수 식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 이렇게 모든 변수들을 하나 하나 선언하고, 곱하고, 더해야 한다.\n",
    " \n",
    " \n",
    " - 하지만 텐서플로의 Dense를 이용하면 한 줄로 이 코드들을 대신할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값에 대한 활성화 함수로 시그모이드 함수를 사용하고, 출력 값으로 10개의 값을 출력하는 완전 연결 계층은 다음과 같이 정의하면 된다.\n",
    "\n",
    "INPUT_SIZE = (20,1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 10개의 노드를 가지는 은닉층이 있고, 최종 출력값은 2개의 노드가 있는 신경망 구조를 만든다면\n",
    "\n",
    "INPUT_SIZE = (20,1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(input)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Dropout\n",
    " - 신경망 모델을 만들 때 생기는 여러 문제중 가장 대표적인 문제는 과적합(Overfitting)이다.\n",
    " - 과적합 문제는 정규화(Regularization) 방법을 사용해서 해결하는데, 그 중 가장 대표적인 방법이 드롭아웃(Dropout)이다.\n",
    " - 쉽게 말해, 드롭아웃이란 확률적으로 인공신경망의 뉴런을 사용하지 않는 것이다.\n",
    " - 텐서플로는 드롭아웃을 쉽게 모델에 적용할 수 있게 간단한 모듈을 제공한다.\n",
    " - tf.keras.layers.Dropout()\n",
    " - 그러나, 드롭아웃은 학습시킬 때만 적용해야지 예측이나 테스트 할 때는 적용하면 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50%를 기준으로 드롭아웃하기.\n",
    "\n",
    "INPUT_SIZE = (20,1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로에서 드롭아웃은 tf.keras.layers.Dropout으로 가능하지만, tf.nn.dropout으로도 가능하다. 차이점은 keras는 rate=0.2 로 지정하면, 20%를 0으로 만들지만, nn은 rate=0.2로 지정하면, 20%를 1로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20,1)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)                               # 20개의 입력값을 받아서,\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)                                 # 20%의 확률로 드롭아웃을 시키고,\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)      # 10개의 노드를 가지는 은닉층을 만들고,\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)        # 최종 결과값은 2개의 노드가 있는 신경망을 구축한다.\n",
    "                                                                                     # 활성화 함수는 모두 시그모이드를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.Conv1D\n",
    " - 텐서플로의 합성곱 연산은 Conv1D, Conv2D, Conv3D로 나눠진다.\n",
    " - 우리가 흔히 알고 있는 기본적인 이미지에 적용하는 방식은 Conv2D다.\n",
    " - 합성곱은 일반적으로 두가지 기준으로 구분하는데, 합성곱의 진행 방향과 합성곱 결과로 나오는 출력값이다.\n",
    " \n",
    "   - Conv1D : 한 방향(가로) + 1-D Array(vector)\n",
    "   - Conv2D : 두 방향(가로,세로) + 2-D Array(matrix)\n",
    "   - Conv3D : 세 방향(가로,세로,높이) + 3-D Array(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D를 이용한 합성공 신경망\n",
    " \n",
    "INPUT_SIZE = (1,28,28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=INPUT_SIZE)\n",
    "\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "                       filters = 10,                           # 필터의 개수로써, 정수형으로 지정한다. 출력의 차원수를 나타낸다.\n",
    "                       kernel_size = 3,                        # 필터의 크기로써, 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. \n",
    "                                                               # 합성곱이 적용되는 윈도우(window)의 길이를 나타낸다. \n",
    "                       padding = \"same\",                       # 패딩 방법을 지정한다. same과 valid 방식이 있다.\n",
    "                       activation = tf.nn.relu)(input)         # 활성화 함수는 relu로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력값에 Dropout을 적용한 신경망\n",
    "\n",
    "INPUT_SIZE = (1,28,28)\n",
    "is_trainging = True                                            # 학습 시작!\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape=INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "                       filters = 10,                           # 필터의 개수로써, 정수형으로 지정한다. 출력의 차원수를 나타낸다.\n",
    "                       kernel_size = 3,                        # 필터의 크기로써, 정수 혹은 정수의 리스트, 튜플 형태로 지정한다. \n",
    "                                                               # 합성곱이 적용되는 윈도우(window)의 길이를 나타낸다. \n",
    "                       padding = \"same\",                       # 패딩 방법을 지정한다. same과 valid 방식이 있다.\n",
    "                       activation = tf.nn.relu)(dropout)       # 활성화 함수는 relu로 지정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.MaxPool1D\n",
    " - 합성공 신경망과 함께 스이는 기법 중 하나는 풀링이다.\n",
    " - 보통 피처 맵의 크기를 줄이거나 주요한 특징을 뽑아내기 위해 합성곱 이후에 적용되는 기법이다.\n",
    " - 풀링에는 맥스 풀링(max-pooling)과 평균 풀링(average-pooling)이 있다.\n",
    " - 맥스 풀링은 피처 맵에 대해 최댓값만 뽑아내는 방식이고, 평균 풀링은 피처 맵에 대해 전체 값들을  평균한 값을 뽑는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 합성공 신경망에서 맥스 풀링을 사용한 후 완전 연결 계층을 통해 최종 출력값이 나오는 구조,\n",
    "# 입력값에는 Dropout을 적용하고, 맥스 풀링 결과값을 완전 연결 계층으로 연결하기 위해서는 행렬이었던 것을 벡터로 만들어야한다.\n",
    "\n",
    "INPUT_SIZE = (1,28,28)\n",
    "\n",
    "input = tf.placeholder(tf.float32, shape = INPUT_SIZE)\n",
    "\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(input)\n",
    "\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(dropout)\n",
    "\n",
    "max_pool = tf.keras.layers.MaxPool1D(pool_size = 3, padding = 'same')(conv)\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)                                         # 행렬을 벡터로 변환시킨다.\n",
    "\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data\n",
    " - 머신러닝 프로젝트에서 가장 많은 시간을 할애하는 부분은 데이터를 다루는 분야이다.\n",
    " - 일반적으로 데이터 분석, 전처리, 파이프라인을 만드는 과정에서 약 70~80%의 시간을 소비한다. \n",
    " - 텐서플로는 Dataset API를 활용해 단순히 연구뿐만 아니라 서비스화를 위해 데이터에 대한 고민들을 해결해준다.\n",
    " - 학습 속도를 올리기 위해 GPU가 병목 현상 없이 효율적으로 처리할 수 있게 최적화를 지원하는 것뿐만 아니라 이미지, 텍스트를 넘어\n",
    " - 넘파이(Numpy) 및 판다스(Pandas) 라이브러리의 데이터 구조를 활용할 수 있게 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리를 위한 모듈 preprocessing\n",
    "\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다', \n",
    "           '나는 오늘 기분이 더러워', \n",
    "           '끝내주는데, 좋은 일이 있나봐', \n",
    "           '나 좋은 일이 생겼어', \n",
    "           '아 오늘 진짜 짜증나', \n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터와 각 데이터의 라벨을 정의했다. 라벨은 임의로 설정한 것이므로 특별한 의미는 없다.\n",
    " \n",
    " \n",
    "다음과 같은 구조로 전처리를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수치화된 텍스트 데이터 : \n",
      " [[4, 1, 5, 6], [7, 1, 8, 9], [10, 2, 3, 11], [12, 2, 3, 13], [14, 1, 15, 16], [17, 18, 19, 20]]\n",
      "\n",
      "각 단어의 인덱스 : \n",
      " {'오늘': 1, '좋은': 2, '일이': 3, '너': 4, '이뻐': 5, '보인다': 6, '나는': 7, '기분이': 8, '더러워': 9, '끝내주는데': 10, '있나봐': 11, '나': 12, '생겼어': 13, '아': 14, '진짜': 15, '짜증나': 16, '환상적인데': 17, '정말': 18, '좋은거': 19, '같아': 20}\n",
      "\n",
      "라벨 : \n",
      " [[1], [0], [1], [1], [0], [1]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트로 구성된 데이터에서 각 단어를 단어의 인덱스로 바꿨다. 즉 텍스트 데이터를 수치화하였다.\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다', \n",
    "           '나는 오늘 기분이 더러워', \n",
    "           '끝내주는데, 좋은 일이 있나봐', \n",
    "           '나 좋은 일이 생겼어', \n",
    "           '아 오늘 진짜 짜증나', \n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "label = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(\"수치화된 텍스트 데이터 : \\n\", sequences)\n",
    "print(\"\")\n",
    "print(\"각 단어의 인덱스 : \\n\", word_index)\n",
    "print(\"\")\n",
    "print(\"라벨 : \\n\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 텍스트로 구성된 데이터가 정수 배열로 바뀌었다.\n",
    " \n",
    " \n",
    "여기서 각 정숫값은 각 단어의 인덱스를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# tf.data를 활용한 처리방법\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences,label))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "# 우선 tf.data.Dataset.from_tensor_slices 함수는 중진 데이터 sequences와 label을 묶어서 조각으로 만들고 함께 사용 가능하게 한다.\n",
    "\n",
    "# 그 다음으로 이렇게 정의한 데이터셋에 대해 make_one_shot_iterator 함수를 사용해서 데이터를 하나씩 사용할 수 있게 만든다.\n",
    "\n",
    "# 이제 데이터가 이터레이터 형식으로 되어 있는데, 이터레이터의 get_next 함수를 사용하면 데이터가 하나씩 나오게 되는 구조이다.\n",
    "\n",
    "with tf.Session() as sess : \n",
    "    while True : \n",
    "        try : print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError : \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 tf.data를 사용하면 간단하게 데이터를 모델에 적용할 수 있다. 뿐만 아니라, 셔플, 배치, 반복 등 여러 기능들을 간단히 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[4, 1, 5, 6],\n",
      "       [7, 1, 8, 9]]), array([[1],\n",
      "       [0]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[14,  1, 15, 16],\n",
      "       [17, 18, 19, 20]]), array([[0],\n",
      "       [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 배치 크기를 정의한 후 데이터가 배치 크기 만큼 한 번에 사용\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([4, 1, 5, 6]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# 데이터 셔플\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n",
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# 데이터 에폭(Epoch) : 전체 데이터를 몇 번 사용하는가..?\n",
    "\n",
    "\n",
    "EPOCH = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([4, 1, 5, 6]), array([1]))\n",
      "(array([7, 1, 8, 9]), array([0]))\n",
      "(array([10,  2,  3, 11]), array([1]))\n",
      "(array([12,  2,  3, 13]), array([1]))\n",
      "(array([14,  1, 15, 16]), array([0]))\n",
      "(array([17, 18, 19, 20]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "# 여러 데이터를 하나로 묶기\n",
    "# 입력 데이터가 \"x\"라는 키 값을 가지고 있는 딕셔너리 구조\n",
    "\n",
    "def mapping_fn(X, Y=None) : \n",
    "    input = {\"x\" : X}\n",
    "    label = Y\n",
    "    return input, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataest = dataset.map(mapping_fn)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess : \n",
    "    while True : \n",
    "        try : \n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError : \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 두 개 이상의 입력값이 존재한다면...?\n",
    "\n",
    "def mapping_fn(X1, X2, Y=None) : \n",
    "    input = {\"x1\" : X1, \"x2\" : X2}\n",
    "    label = Y\n",
    "    return input, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch, Shuffle, Epoch, Mapping 한 번에 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 4,  1,  5,  6],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[ 7,  1,  8,  9],\n",
      "       [14,  1, 15, 16]]), array([[0],\n",
      "       [0]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [17, 18, 19, 20]]), array([[1],\n",
      "       [1]]))\n",
      "(array([[14,  1, 15, 16],\n",
      "       [ 4,  1,  5,  6]]), array([[0],\n",
      "       [1]]))\n",
      "(array([[17, 18, 19, 20],\n",
      "       [ 7,  1,  8,  9]]), array([[1],\n",
      "       [0]]))\n",
      "(array([[10,  2,  3, 11],\n",
      "       [12,  2,  3, 13]]), array([[1],\n",
      "       [1]]))\n"
     ]
    }
   ],
   "source": [
    "# 매치 크기가 2, 에폭 수가 2, 셔플과 매핑을 적용해 데이터를 불러오기\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCH = 2\n",
    "\n",
    "def mapping_fn(X, Y=None) : \n",
    "    input = {\"x\" : X}\n",
    "    label = Y\n",
    "    return input, label\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, label))\n",
    "dataest = dataset.map(mapping_fn)\n",
    "dataset = dataset.shuffle(len(sequences))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.repeat(EPOCH)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_data = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess : \n",
    "    while True : \n",
    "        try : \n",
    "            print(sess.run(next_data))\n",
    "        except tf.errors.OutOfRangeError : \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator(에스티메이터)\n",
    " - 텐서플로는 다양한 머신러닝과 딥러닝 모델을 구현할 때 연산을 유연하고 엄밀하게 수행할 수 있도록 그래프와 세션과 같은 기능을 제공한다.\n",
    " - 하지만, 모델을 의미하는 그래프를 따로 구현하고 세션을 사용해서 모델을 실행하는 방식이 초보자들에게는 직관적이지 않고 사용하기 어려울 수 있다는 단점이 있다.\n",
    " - 또한 딥러닝 모델을 최대한 빠르게 실험해보고 다양한 모델을 적용해야 하는 연구자와 개발자들은 다른 부분들 보다는 모델 구현에 집중할 수 있는 환경이 중요하다.\n",
    " - 이러한 것들을 충족시켜줄 수 있는 기능이 바로 텐서플로의 에스티메이터다.\n",
    " \n",
    "\n",
    "#### 에스티메이터에서 기본적으로 제공하는 기능\n",
    " - 학습(Train) : 정의한 모델 파라미터에 대해 학습한다.\n",
    " - 평가(Evalutate) : 학습한 모델에 대한 성능을 측정한다.\n",
    " - 예측(Predict) : 모델을 통해 입력값에 대한 예측값을 받는다.\n",
    " - 배포(Export) : 사용할 모델을 바이너리 파일로 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에스티메이터 준비사항\n",
    " - 에스티데이터에는 선형 회귀, 선형 분류, 심층 신경망 분류기, 심층 신경망 회귀 모델 등 기본적인 모델이 이미 구현되어 있다.\n",
    " - 에스티메이터를 구현하기 위해서는 기본적으로 두 가지 함수를 구현해야 한다.\n",
    "   - 1. 사용할 모델을 구현한 모델 함수\n",
    "   - 2. 모델에 적용될 데이터를 에스티메이터에 전달하기 위한 데이터 입력 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 에스티메이터를 활용한 심층 신경망 분류기 구현\n",
    " - DNN(Depp Neural Network)를 사용한 텍스트의 긍/부정을 예측하는 감정 분석 모델(Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './data_outcheckpoint/dnn', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020BD733F4A8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./data_outcheckpoint/dnn\\model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into ./data_outcheckpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 9.609057e-07, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 882.213\n",
      "INFO:tensorflow:loss = 3.259001e-06, step = 3101 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 1402.41\n",
      "INFO:tensorflow:loss = 2.6454748e-06, step = 3201 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1155.25\n",
      "INFO:tensorflow:loss = 2.440028e-06, step = 3301 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1224.6\n",
      "INFO:tensorflow:loss = 6.9454245e-07, step = 3401 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1209.98\n",
      "INFO:tensorflow:loss = 6.593344e-07, step = 3501 (0.083 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3600 into ./data_outcheckpoint/dnn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.111714e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x20bd733f2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "samples = ['너 오늘 이뻐 보인다', \n",
    "           '나는 오늘 기분이 더러워', \n",
    "           '끝내주는데, 좋은 일이 있나봐', \n",
    "           '나 좋은 일이 생겼어', \n",
    "           '아 오늘 진짜 짜증나', \n",
    "           '환상적인데, 정말 좋은거 같아']\n",
    "\n",
    "labels = [[1], [0], [1], [1], [0], [1]]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPOCH = 100\n",
    "\n",
    "def train_input_fn() : \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "    dataset = dataset.repeat(EPOCH)\n",
    "    dataset = dataset.batch(1)\n",
    "    dataset = dataset.shuffle(len(sequences))\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "EMB_SIZE = 128\n",
    "\n",
    "def model_fn(features, labels, mode) : \n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embed_input = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features)\n",
    "    embed_input = tf.reduce_mean(embed_input, axis=1)\n",
    "    \n",
    "    hidden_layer = tf.keras.layers.Dense(128, activation=tf.nn.relu)(embed_input)\n",
    "    output_layer = tf.keras.layers.Dense(1)(hidden_layer)\n",
    "    output = tf.nn.sigmoid(output_layer)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output, labels)\n",
    "    \n",
    "    if TRAIN : \n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, train_op = train_op, loss = loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "DATA_OUT_PATH = \"./data_out\"\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH) :\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "       \n",
    "estimator = tf.estimator.Estimator(model_fn = model_fn, model_dir = DATA_OUT_PATH + \"checkpoint/dnn\")\n",
    "estimator.train(train_input_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
