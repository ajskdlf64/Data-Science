{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런\n",
    "scikit-learn은 파이썬용 머신러닝 라이브러리다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset key : dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기 및 딕셔너리의 키값 확인\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_dataset = load_iris()\n",
    "print(\"iris_dataset key : {}\" .format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "shape of data : (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인 및 데이터 형태 보기\n",
    "\n",
    "print(iris_dataset[\"data\"])\n",
    "print(\"shape of data : {}\" .format(iris_dataset[\"data\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# Feature 변수명 확인\n",
    "print(iris_dataset[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Target 확인\n",
    "print(iris_dataset[\"target\"])\n",
    "print(iris_dataset[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# 데이터에 대한 전체적인 요약 정보\n",
    "print(iris_dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런을 이용한 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_input : (112, 4)\n",
      "shape of train_label : (112,)\n",
      "shape of test_input : (38, 4)\n",
      "shape of test_label : (38,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset[\"data\"],iris_dataset[\"target\"], test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(\"shape of train_input : {}\" .format(train_input.shape))\n",
    "print(\"shape of train_label : {}\" .format(train_label.shape))\n",
    "print(\"shape of test_input : {}\" .format(test_input.shape))\n",
    "print(\"shape of test_label : {}\" .format(test_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런을 이용한 지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  K-nearest neighbor classifier : K-최근접 이웃 분류기\n",
    " - 데이터에 대한 가정이 없어 단순하다.\n",
    " - 다목적 분류와 회귀에 좋다.\n",
    " - 높은 메모리를 요구한다.\n",
    " - k값이 커지면 계산이 늦어질 수 있다.\n",
    " - 관련 없는 기능의 데이터의 규모에 민감하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy : 1.00\n"
     ]
    }
   ],
   "source": [
    "# K-최근접 이웃 분류기 생성하기\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "# 모델에 훈련 데이터(75%) 적합시키기\n",
    "knn.fit(train_input, train_label)\n",
    "\n",
    "# 새로운 데이터 생성\n",
    "import numpy as np\n",
    "new_input = np.array([[6.1,2.8,4.7,1.2]])\n",
    "\n",
    "# 새로운 데이터에 대한 예측값\n",
    "knn.predict(new_input)\n",
    "\n",
    "# 테스트 데이터(25%)에 대한 예측값\n",
    "knn.predict(test_input)\n",
    "\n",
    "# 예측 정확도\n",
    "print(\"test accuracy : {:.2f}\" .format(np.mean(knn.predict(test_input) == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런을 이용한 비지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans Cluster\n",
    " - 초기값을 랜덤으로 설정하기 때문에 매번 결과가 바뀔 수 있다.\n",
    " - 0번 군집을 \"1\" 품종으로, 1번 군집을 \"0\" 품종으로, 2번 군집을 \"2\" 품종으로 라벨링 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 2 2 1 1 2 2 0 2 0 2 0 2 1 0 2 1 1 1 2 2 1 1 1 2 1 2 0 1 2 2 1 2 2 2\n",
      " 2 0 2 1 2 0 1 1 2 0 1 2 1 1 2 2 0 2 0 0 2 1 1 2 0 1 1 1 2 0 1 0 0 1 2 2 2\n",
      " 0 0 1 0 2 0 2 2 2 1 2 2 1 2 0 0 1 2 0 0 1 0 1 0 0 0 2 0 2 2 2 2 1 2 2 1 2\n",
      " 0]\n",
      "\n",
      "0 Cluster : [2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2]\n",
      "1 Cluster : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "2 Cluster : [2 1 1 1 2 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 1 2 1]\n",
      "\n",
      "새로운 데이터에 대한 예측값 :  [2]\n",
      "\n",
      "테스트 데이터에 대한 예측값 :  [2 1 0 2 2 1 2 0 2 2 0 1 1 1 1 2 0 2 2 0 1 2 1 0 0 0 0 0 1 1 1 1 2 1 1 2 2\n",
      " 1]\n",
      "\n",
      "바뀐 라벨 :  [2, 0, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0, 0, 0, 0, 2, 1, 2, 2, 1, 0, 2, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0]\n",
      "\n",
      "test accuracy : 0.45\n"
     ]
    }
   ],
   "source": [
    "# K-평균 군집화 모델\n",
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters = 3)\n",
    "\n",
    "# 훈련데이터(75%) 적합시키기\n",
    "k_means.fit(train_input)\n",
    "\n",
    "# 적합결과 확인하기\n",
    "print(k_means.labels_)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 잘 예측이 되었는지, 각 군집의 종 확인하기\n",
    "print(\"0 Cluster :\", train_label[k_means.labels_ == 0])\n",
    "print(\"1 Cluster :\", train_label[k_means.labels_ == 1])\n",
    "print(\"2 Cluster :\", train_label[k_means.labels_ == 2])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 새로운 데이터 생성하기\n",
    "import numpy as np\n",
    "new_input = np.array([[6.1,2.8,4.7,1.2]])\n",
    "\n",
    "# 새로운 데이터에 대한 예측값\n",
    "prediction = k_means.predict(new_input)\n",
    "print(\"새로운 데이터에 대한 예측값 : \",prediction)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 테스트 데이터(25%)에 대한 예측값\n",
    "predict_cluster = k_means.predict(test_input)\n",
    "print(\"테스트 데이터에 대한 예측값 : \",predict_cluster)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 각 군집에 알맞는 품종 라벨링하기\n",
    "np_arr = np.array(predict_cluster)\n",
    "np_arr[np_arr==0], np_arr[np_arr==1], np_arr[np_arr==2] = 3, 4, 5\n",
    "np_arr[np_arr==3] = 1\n",
    "np_arr[np_arr==4] = 0\n",
    "np_arr[np_arr==5] = 2\n",
    "predict_label = np_arr.tolist()\n",
    "print(\"바뀐 라벨 : \", predict_label)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 예측 정확도\n",
    "print(\"test accuracy : {:.2f}\" .format(np.mean(predict_label == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사이킷런을 이용한 특징 추출\n",
    " - 자연어 처리에서 특징추출이란 텍스트 데이터에서 단어나 문장들을 어떤 특징 값으로 바꿔주는 것을 의미한다.\n",
    " - 기존에 문자로 구성되어 있던 데이터를 모델에 적용할 수 있도록 특징을 뽑아 어떤 값으로 수치화한다.\n",
    " \n",
    "\n",
    "#### 특징화 세 가지 방법\n",
    " - 1. CountVectorizer\n",
    " - 2. TfidVectorizer\n",
    " - 3. HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - CountVectorizer : 단순히 각 텍스트에서 회수를 기준으로 특징을 추출하는 방법\n",
    " - TfidVectorizer : TF-IDF라는 값을 사용해서 텍스트에서 특징을 추출한다.\n",
    " - HashingVectorizer : CountVectorizer와 동일한 방법이지만 텍스트를 처리할 때 해시 함수를 사용해서 실행 시간을 현저하게 줄인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer\n",
    " - 텍스트 데이터에서 횟수를 기준으로 특징을 추출하는 방법\n",
    " - 어떤 단위의 횟수를 셀 것인지는 선택사항이다.\n",
    " - 보통은 텍스트에서 단어를 기준으로 횟수를 측정하는데, 문장을 입력으로 받아 단어의 횟수를 측정한 뒤 벡터로 만든다.\n",
    " - \"나는 배가 고프다\"라는 문장을 벡터로 만들었다. 각 단어가 1번 씩 나왔으므로 해당 단어 사전 순서에 맞게 1값을 가진다.\n",
    " - 횟수를 사용해서 벡터를 만들기 때문에 직관적이고 간단해서 여러 상황에서 사용할 수 있다는 장점이다\n",
    " - 하지만, 단순히 횟수만을 특징으로 잡기 때문에 큰 의미가 없지만 자주 사용되는 단어들, 예를 들면 조사 혹은 지시대명사가 높은 특징값을 가지기 때문에 유의미하게 사용하기 어려울 수 있다.\n",
    " - 이러한 문제점을 해결할 수 있는 TF-IDF 방식이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
      "\n",
      "나는 배가 고프다\n",
      "[[1 0 1 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer 구현하기\n",
    "\n",
    "# 연습 데이터 생성하기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = [\"나는 배가 고프다\", \"내일 점심 뭐먹지\", \"내일 공부 해야겠다\", \"점심 먹고 공부 해야지\"]\n",
    "\n",
    "# 단어 사전 만들기\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(text_data)\n",
    "print(count_vectorizer.vocabulary_)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 텍스트 데이터를 벡터로 만들기\n",
    "sentence = [text_data[0]]\n",
    "print(\"나는 배가 고프다\")\n",
    "print(count_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVecotorizer\n",
    " - TFidVectorizer는 TF-IDF라는 ㅌ특정한 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법이다.\n",
    " - 각 값이 의미하는 바를 간단히 설명하면 TF(Term Frequency)란 특정 단어가 하나의 데이터 안에서 등장하는 횟수를 의미한다.\n",
    " - 그리고 DF(Document Frequency)는 문서 빈도 값으로, 특정 단어가 여러 데이터에 자주 등장하는지를 알려주는 지표이다.\n",
    " - IDF(Inverse Document Frequency)는 이 값에 역수를 취해서 구할 수 있으며, 특정 단어가 다른 데이터에 등장하지 않을수록 값이 커진다.\n",
    " - TF-IDF란 이 두 값을 곱해서 사용하므로 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 없는 단어일수록 높은 값을 가지게 된다. 따라서 조사나 지시대명사처럼 자주 등장하는 단어는 TF값은 크지만 IDF값은 작아지므로 CountVectorizer가 가진 문제점을 해결한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
      "\n",
      "점심 먹고 공부해야지\n",
      "[[0.         0.43779123 0.         0.         0.55528266 0.\n",
      "  0.         0.43779123 0.         0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer 구현하기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = [\"나는 배가 고프다\", \"내일 점심 뭐먹지\", \"내일 공부 해야겠다\", \"점심 먹고 공부 해야지\"]\n",
    "\n",
    "# 단어 사전 구현하기\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# 텍스트 데이터를 벡터로 만들기\n",
    "sentence = [text_data[3]]\n",
    "print(\"점심 먹고 공부해야지\")\n",
    "print(tfidf_vectorizer.transform(sentence).toarray())\n",
    "\n",
    "# \"먹고\"와 \"해야지\"가 조금 더 높은 이유는 다른 데이터에서는 잘 나오지 않는 단어이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 특징 추출 방법으로 TF-IDF 값을 사용할 경우, 단순 횟수를 이용하는 것보다 각 단어의 특성을 좀 더 잘 반영할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
